{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All code written by Amirsadra Mohseni\n",
    "Supplemental material provided by UC Riverside\n",
    "\n",
    "Requires pandas and numpy packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classify(test_data, training_data, training_labels, k=1):\n",
    "    # Distance b/w test instance and all training data instances\n",
    "    dist = np.linalg.norm(test_data - training_data, axis=1)\n",
    "\n",
    "    distance_label_pairs = dict(zip(dist, training_labels)) # Assign distances with labels (distance : label)\n",
    "    distance_label_pairs = sorted(distance_label_pairs.items()) # sort based on keys (distances)\n",
    "\n",
    "    k_nearest = distance_label_pairs[0:k] # Only need the first k pairs\n",
    "\n",
    "    # Return the most common label in distance:label pairs\n",
    "    return Counter([pair[1] for pair in k_nearest]).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-One-Out Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_out_cross_validation(data, all_labels, list_of_features, feature_to_add=None, k=1):\n",
    "    all_accuracy = []\n",
    "    features = []\n",
    "    features = list(list_of_features) # Make a copy before append\n",
    "    \n",
    "    if (feature_to_add != None):\n",
    "        features.append(feature_to_add)\n",
    "    \n",
    "    for i in range(0, len(data)):\n",
    "        # Pick out the i-th row and the desired feature columns\n",
    "        test_instance = data.iloc[i, features]\n",
    "        test_label = all_labels[i]\n",
    "        \n",
    "        # Drop the i-th row, select all rows (except the i-th which is dropped) and the desired feature columns\n",
    "        train_instances = data.drop(i).iloc[:, features]\n",
    "        train_labels = list(all_labels.drop(i))\n",
    "\n",
    "        prediction = knn_classify(test_instance, train_instances, train_labels, k)\n",
    "        \n",
    "        if (prediction == test_label):\n",
    "            all_accuracy.append(1) # Correct pred\n",
    "        else:\n",
    "            all_accuracy.append(0) # Incorrect pred\n",
    "    \n",
    "    return sum(all_accuracy)/len(all_accuracy) # Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_search(data, all_labels, k, verbose=1):\n",
    "    set_of_features = []\n",
    "    subsets = []\n",
    "    subsets_accuracy = []\n",
    "    \n",
    "    print(\"Beginning search...\")\n",
    "    \n",
    "    for i in range(0, len(data.columns)):\n",
    "        if verbose: print(\"Level \" + str(i) + \" of the search tree\")\n",
    "        else: print(\"[\" + str(i) + \"]/[\" + str(len(data.columns) - 1) + \"] Working...\")\n",
    "        \n",
    "        this_level_feature = 0\n",
    "        best_accuracy_so_far = 0\n",
    "        \n",
    "        for feature_to_add in range(0, len(data.columns)):\n",
    "            \n",
    "            if (feature_to_add in set_of_features):\n",
    "                # only consider adding if not already added\n",
    "                continue\n",
    "            \n",
    "            if verbose: print(\"--Considering adding feature \" + str(feature_to_add))\n",
    "            accuracy = leave_one_out_cross_validation(data, all_labels, set_of_features, feature_to_add, k)\n",
    "            if verbose: print(\"----Accuracy of feature \" + str(feature_to_add) + \": \" + str(round(accuracy, 2)))\n",
    "            \n",
    "            if (accuracy > best_accuracy_so_far):\n",
    "                best_accuracy_so_far = accuracy\n",
    "                this_level_feature = feature_to_add\n",
    "                \n",
    "        set_of_features.append(this_level_feature)\n",
    "        \n",
    "        subsets.append(list(set_of_features))\n",
    "        subsets_accuracy.append(best_accuracy_so_far)\n",
    "        \n",
    "        if verbose: print(\"On level \" + str(i) + \" added feature \" + str(this_level_feature) + \" to current set\\n\\n\")\n",
    "    \n",
    "    most_accuracy = max(subsets_accuracy)\n",
    "    most_accurate_subset = subsets[subsets_accuracy.index(most_accuracy)]\n",
    "    \n",
    "    print(\"Most accurate subset is \" + str(most_accurate_subset) + \" with an accuracy of \" + str(most_accuracy * 100) + \"%\")\n",
    "    #print(\"All subsets: \" + str(subsets))\n",
    "    #print(\"All accuracies: \" + str(subsets_accuracy))\n",
    "    \n",
    "    return most_accurate_subset, most_accuracy, subsets, subsets_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Welcome to Amir's Feature Selection Algorithm\")\n",
    "    file_name = input(\"Type in the name of the file to test or Enter nothing for small80.txt: \") or \"small80.txt\"\n",
    "    \n",
    "    num_cols = len(pd.read_csv(file_name, nrows=1, delim_whitespace=True).columns) - 1\n",
    "    col_names=['CLASS']\n",
    "\n",
    "    for i in range(num_cols):\n",
    "        col_names.append(\"F\" + str(i))\n",
    "\n",
    "    df = pd.read_csv(file_name, delim_whitespace=True, names=col_names)\n",
    "    all_vals = df.drop(\"CLASS\", axis=1)\n",
    "    all_labels = df[\"CLASS\"]\n",
    "    \n",
    "    print(\"\\nThis dataset has \" + str(num_cols) + \" features (not including the CLASS attribute), with \" + str(len(df)) + \" instances.\")\n",
    "    norm_yes_no = int(input(\"Normalize data? 1 for yes, 0 for no. Enter nothing for default yes (z-score norm)\") or 1)\n",
    "    \n",
    "    # z-score normalize\n",
    "    if norm_yes_no:\n",
    "        print(\"Please wait while I z-score normalize the data... \", end=\"\")\n",
    "        all_vals = (all_vals - all_vals.mean()) / all_vals.std(ddof=0)\n",
    "        print(\"Done.\")\n",
    "    \n",
    "    print(\"Using Forward Selection algorithm\")\n",
    "    \n",
    "    k = int(input(\"\\nWhich k would you like to use with KNN? Enter nothing for default 1: \") or 1)\n",
    "    \n",
    "    verbose = int(input(\"\\nShow detailed search tree info? Enter 1 for yes, and 0 for no. Enter nothing for default 1: \") or 1)\n",
    "    \n",
    "    acc = leave_one_out_cross_validation(all_vals, all_labels, [x for x in range(0, num_cols)], None, k=k)\n",
    "    \n",
    "    print(\"\\nRunning nearest neighbor with all \" + str(num_cols) +\n",
    "          \" features, using “leaving-one-out” evaluation, I get an accuracy of \" + str(acc) + \"\\n\")\n",
    "    \n",
    "    t1_start = perf_counter()\n",
    "    #-----------------TIME-----------------\n",
    "    feature_search(all_vals, all_labels, k, verbose=verbose)\n",
    "    #-----------------TIME-----------------\n",
    "    t1_stop = perf_counter()\n",
    "    s = (t1_stop - t1_start)\n",
    "    print(\"--- %s seconds ---\" % s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Amir's Feature Selection Algorithm\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type in the name of the file to test or Enter nothing for small80.txt:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This dataset has 10 features (not including the CLASS attribute), with 100 instances.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Normalize data? 1 for yes, 0 for no. Enter nothing for default yes (z-score norm) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait while I z-score normalize the data... Done.\n",
      "Using Forward Selection algorithm\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Which k would you like to use with KNN? Enter nothing for default 1:  \n",
      "\n",
      "Show detailed search tree info? Enter 1 for yes, and 0 for no. Enter nothing for default 1:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running nearest neighbor with all 10 features, using “leaving-one-out” evaluation, I get an accuracy of 0.65\n",
      "\n",
      "Beginning search...\n",
      "Level 0 of the search tree\n",
      "--Considering adding feature 0\n",
      "----Accuracy of feature 0: 0.57\n",
      "--Considering adding feature 1\n",
      "----Accuracy of feature 1: 0.54\n",
      "--Considering adding feature 2\n",
      "----Accuracy of feature 2: 0.68\n",
      "--Considering adding feature 3\n",
      "----Accuracy of feature 3: 0.65\n",
      "--Considering adding feature 4\n",
      "----Accuracy of feature 4: 0.75\n",
      "--Considering adding feature 5\n",
      "----Accuracy of feature 5: 0.61\n",
      "--Considering adding feature 6\n",
      "----Accuracy of feature 6: 0.62\n",
      "--Considering adding feature 7\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
